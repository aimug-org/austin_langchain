{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codestral Self Correction Agent with Ollama\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_108/codestral_self_correction_ollama.ipynb)\n",
    "\n",
    "This notebook adapts the [original notebook](https://colab.research.google.com/drive/12wc4SND1bY52H7A7EJkhgWDwqmWcvy5k?usp=sharing) and featured on this [YouTube video](https://www.youtube.com/watch?v=zXFxmI9f06M) to work with [Ollama](https://ollama.com/) using [OllamaFunctions](https://python.langchain.com/v0.2/docs/integrations/chat/ollama_functions/) module within [langchain-experimental](https://pypi.org/project/langchain-experimental/) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jI7WuezVNW7M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! pip install -U langchain_community langchain-mistralai langchain langgraph\n",
    "! pip install -U langchain_community langchain_experimental langchain langgraph httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optionally) Setup environment variables for LangSmith tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "58NlrpCANZL8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['MISTRAL_API_KEY'] = 'xxx'\n",
    "\n",
    "# Uncomment below for LangSmith tracing and include your LANGCHAIN_API_KEY.\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_API_KEY'] = \"xxx\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Mistral-code-gen-testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and run Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "ollama serve &> ollama.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for Ollama service to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!until ollama list; do echo waiting for ollama; sleep 5; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Codestral model for Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "ollama pull codestral >> codestral.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the model to be downloaded and primed. This may take a couple of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo waiting for codestral\n",
    "!until ollama list | grep codestral; do sleep 5; done\n",
    "!echo priming ollama for codestral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "ollama run codestral \"How to print hello world in python?\" > sample.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat sample.md\n",
    "echo\n",
    "echo codestral should be ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and test Codestral using OllamaFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMkyCvtONtvG",
    "outputId": "91a9f41a-0b96-48fb-cb79-5c8210f4a145"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "code(prefix=\"To write a function for calculating Fibonacci numbers, we can use recursion or iteration. Here, I'll use the iterative approach as it is more efficient.\", imports='', code='def fibonacci(n):\\n  if n <= 0:\\n    return \"Input should be a positive integer\"\\n  elif n == 1:\\n    return 0\\n  elif n == 2:\\n    return 1\\n  else:\\n    a, b = 0, 1\\n    for _ in range(2, n):\\n      a, b = b, a + b\\n    return b', description='Schema for code solutions to questions about LCEL.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select LLM\n",
    "#from langchain_mistralai import ChatMistralAI\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# mistral_model = \"codestral-latest\"\n",
    "mistral_model = \"codestral:latest\"\n",
    "llm = OllamaFunctions(model=mistral_model, temperature=0)\n",
    "\n",
    "# Prompt\n",
    "code_gen_prompt_claude = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant. Ensure any code you provide can be executed with all required imports and variables \\n\n",
    "            defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block.\n",
    "            \\n Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "# LLM\n",
    "code_gen_chain = llm.with_structured_output(code, include_raw=False)\n",
    "\n",
    "# Test\n",
    "question = \"Write a function for fibonacci.\"\n",
    "messages = [(\"user\", question)]\n",
    "result = code_gen_chain.invoke(messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Codestral Agent using LangGraph\n",
    "\n",
    "### Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3VW33LlYNyzV"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing import Dict, TypedDict, List\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Nodes and other related code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MyB3GnDqN2LG"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "### Parameters\n",
    "max_iterations = 3\n",
    "\n",
    "### Nodes\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state[\"error\"]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(messages)\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"Here is my attempt to solve the problem: {code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    prefix = code_solution.prefix\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test. Here is the error: {e}. Reflect on this error and your prior attempt to solve the problem. (1) State what you think went wrong with the prior solution and (2) try to solve this problem again. Return the FULL SOLUTION. Use the code tool to structure the output with a prefix, imports, and code block:\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        combined_code = f\"{imports}\\n{code}\"\n",
    "        # Use a shared scope for exec\n",
    "        global_scope = {}\n",
    "        exec(combined_code, global_scope)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}) Reflect on this error and your prior attempt to solve the problem. (1) State what you think went wrong with the prior solution and (2) try to solve this problem again. Return the FULL SOLUTION. Use the code tool to structure the output with a prefix, imports, and code block:\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "### Conditional edges\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        return \"generate\"\n",
    "\n",
    "### Utilities\n",
    "\n",
    "import uuid\n",
    "\n",
    "def _print_event(event: dict, _printed: set, max_length=1500):\n",
    "    current_state = event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(f\"Currently in: \", current_state[-1])\n",
    "    message = event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "V-4vGOSNN39z",
    "outputId": "0fd30d62-384f-40d2-bfb0-a5034c124f31"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAFBAHsDASIAAhEBAxEB/8QAHQABAAMAAwEBAQAAAAAAAAAAAAUGBwMECAkCAf/EAE8QAAEDAwEDBQwFBwoFBQAAAAECAwQABQYRBxIhCBMWMVUUFRciMkFRYZOU0eE2cXOz0iM1UlSBkZIJM0JWYnR1drGyJCZFU6FXhZWiwf/EABsBAQACAwEBAAAAAAAAAAAAAAACBAEDBQYH/8QAPBEAAgECAQgGBwgBBQAAAAAAAAECAxEhBBITFBUxUVJBU2GRobEFcZLB0uHwIjIzNEJigdFyQ2OCsvH/2gAMAwEAAhEDEQA/APqnSlKA6s66wrZud2TGIm/ru8+6lG9p16anj1j99dXpVZe2IHvKPjVK2kQ487OMZbksNyEC33BQS6gKAPOQ+OhrodHrX2bD9gj4VUynLKOSuMZxbbV8LcWvcdKhkemgp51jROlVl7Yge8o+NOlVl7Yge8o+NZ30etfZsP2CPhTo9a+zYfsEfCqm1cn5Jd6N+zv3eBonSqy9sQPeUfGnSqy9sQPeUfGs76PWvs2H7BHwp0etfZsP2CPhTauT8ku9DZ37vA0TpVZe2IHvKPjTpVZe2IHvKPjWd9HrX2bD9gj4U6PWvs2H7BHwptXJ+SXehs793gaJ0qsvbED3lHxp0qsvbED3lHxrO+j1r7Nh+wR8KdHrX2bD9gj4U2rk/JLvQ2d+7wNFRk9ncWlCLtBUtR0CUyUEk+jrqTrEcpstvjWguNQIzTiZEcpWhlII/LI6iBW3V0qNaGUUlVgmsWsexJ+8o5RQ0DSve4pSlbCoKUpQGd5/9PMa/wANuH3sOuOuTP8A6eY1/htw+9h1x15v0t+LD/H3yPSZF+Av5FQmYZpZcCsi7vfpyYEBK0Nc4UKWpa1HdShCEAqWok6BKQSam6oG262Wq64Rzd2td9uTLUth9leNtKcnxHkK3m5DQT42qCNeAP1HqrjQSlJJ7i5JtRbRBZhykMexuPhkyI1NuduyK4OQ+fat8srYQ2hZcVzQZKysLQE82QFcVEAhCqsOV7ccLwd6I1fLs7AXJjImJCoElQbZUSErdKWyGhqCPym7podeqsjfk5zOwzAMkyGzXi7uY/lrkhaUW7dub9t5qQy1Icio4hz8ogqQka6cdOuv3tYfyLNL1eYsq2ZsqxXHH2u8FusjLsZt2U6hwPJnrSU82pJLQ3HVBG7vcCdauaGDaXrvj2+oraWdm/V0dhr2S7Z8PxK7x7Xcbss3KRDTPYiw4b8tx5hSikLQGUK3xqlXVqQBqeHGonBtuVszXaHlWJohzY0mzzO5GXVQZPNvgNJWtSlloIb0UopCVK8YAKTqFCqVsXx+6M7QMQuM6zXCGiNs3g211+ZEW1zUlD/5RklQGi/F13esjQ9RBqwYO/OxLbbn1unWO7mPkU+PPgXViEtyEW0wm21hx4eK2oLaUNFaE6p066g6cI5yWLtx7SSnN2fRc2GlKVTLRC5f+Y1/bx/vkVsVY7l/5jX9vH++RWxV6/0Z+T/5S8onC9Iffj6hSlK6JyRSlKAzvP8A6eY1/htw+9h1W8r2f4znQi9I7Bbb73Lvcx3wiof5re03t3eB013U66egVo+T4TCyqXClSJEyLIhodbbchvc2d1woKgeHHi2j91RXgqg9sXv335VRyrItZlGpGpmtK258X/Z1qGVU6dJQkrmXjYFs0CCgYFjgQSCU97GdCRrofJ9Z/fUvi+zDEMJnOTcfxi02SY42WVvwIbbK1IJBKSUgEjVIOnqFXnwVQe2L3778qeCqD2xe/fflVJ+i5tWdbzN6y2gsVHwRG0qS8FUHti9++/Ksi27xZuz/ACbZVCtF7uiGMiyhm1Tw7I3yphSFkhJ08U6gcahsf/dXcye0KXBml10rxZoGQ2yRbrpDYuECQnceiyWw424nr0Uk8CKn/BVB7Yvfvvyp4KoPbF799+VNkNf6q7mY1+k+hmXDk/7Mx1YBjY/9rZ/DXPb9h2zy0z406Fg+PxJkZ1LzEhm2tJW04kgpUkhOoIIBBHorSvBVB7Yvfvvyp4KoPbF799+VbNmT67zI65Q5fBFTy/8AMa/t4/3yK2KqQvZLbHSgPXK7yG0rS5zbszVKilQUNRp6QKu9dPJ6CyagqWdd3b70l7ihlVeNeScRSlK3lEUpSgFKUoBSlKAV535V3032Cf56jfdOV6IrzvyrvpvsE/z1G+6coD0RSlKAUpSgFKUoBSlKAUpSgFKUoBSlKAV535V3032Cf56jfdOV6IrzvyrvpvsE/wA9RvunKA9EUpSgFKUoBSlKAUpSgFKUoBSlUi4bS0uuKbsNvN3CToZjjvMxfrSvQlz60pKf7XonGEp7icISm7RVy70rNTmmWqOoh2Vsfolx5Wn7dB/pTpnl36tZP4nq2aJcy7yxqlbgaVXw65U+xR7YLtpvmNBCu9Tiu7bW4ok78Rwko4k6kpIUgk9akGvr30zy79Wsn8T1ZHtu2MHb1f8AD7tkUS0h/HJndCUNFzdltEhSmHdRxQVJSfSBvAabxNNEuZd41StwJXkFbCzsY2GwpM+MWMjyTduc8LTottBH5Bk+cbqDqQeIU4sV6RrNemeXfq1k/iep0zy79Wsn8T1NEuZd41StwNKpWa9M8u/VrJ/E9XMztBv8RW9NscWazrxNulkOgepDiQk/xj4NFwku/wDsPJay/SaJSoywZJAyWIp+C8V82rcdZcSUOsq013VoPFJ048esEEagg1J1qlFxdmiq007MUpSomBSlKAz7Pboq83M462rSA20l247p/nd4+Iwf7JAKljzjdSdUqUK6SUhKQlIAAGgA81dJhancjyhxf84bmUnhx0Sy0lP/ANQP31W9sWXXDAtluUZDaoqZlxtsFyQw0tJUneA8pQHEpT5RA8wNbK+DVNbl5tY/XCx6LJ4xp0k/5LjSvLU7bRkez6Zls8Zm1tItVqxRF0aeZjRmmGprz6W20OFlI4EDfSN4EI39deChYMfybaza7ktdzi3yVZ126W5MmXqBbIyYLyGVLaWwIz7ilJKk7pS4FdYO9wNVrGxVk3azPQtK88YZn2aW5rZFfL7kvfuDmcTSbb+97LKIyzBVJQtpSAF66tkKClEHeJATwAhcH2obWc3i2LLbbarxLt1zlNum1Kh21FsRCU5uq3X+6O6ecS3qreUnQqTpuAHgsNMuD+v/AE9Q0rEMCyTKLzJz7Ib9mve7Hsdvl0htREwWOaTGZSdFvL3N8hvUEbpSTueMVb3Cv7NNouZ3TPoOP3C9XuZaMjssuXb7tdbLEgPNOtlvdejoQVaoKXQd19GuoT1gkUsZ0qww3no1DiXU7yFBadSNUnUag6Gv1XlHZrld+2U8k2xXqHc5V9n3RcWBbIUiPH5uC47JU3qjdDZX5W9o6vQlKRvJBJrTNkt22jqzF+FkkO8ycdXBU6m4X2Jb4z7UoLSA2kRHlhSFJUo+MkEFA4nWlhGqpWw3mrynpNofTeLchS5sZOqmEK0EpoaktK8x6yUk+SrQ9RUDqNunsXW3xpsVwPRZLSXmnE9SkKAKT+0EVnNT+yZalYHASfJadkst/ZokOJRp6t1KatL7VK76Gl33/rxOdl0ErTRb6UpWo5IpSlAZplEBVizB6QQRCvAStKyfFTJQgJKPrUhKSPTuL/bXs+tkm9YRfoENt96XKhPMtNxZfcjqlKQQAl7RXNnj5Wh09BrYLtaYl8tz0GcyH4rwAUgkg6gghQI0KVAgEKBBBAIIIBqgT8XyCxLKY7QyCECAhaFoalJH9sKIQs+sFOv6Pp3OOms08fM6+TZTHM0c3Y84bI9lOTtSLpYcgtEyBs9nW12NLs96kW55bz6ikJUyYTTYQkICwSo66lOgBGo0rFtjTGMRpUVWV5PeYbsFduai3Sel1uOyrQeIAhOqgAAFL3iBqNeJ1uZnXFJ0XjV6SodY7mSr/wAhRFO+E/8Aq5evdPnUdXq8PIuRdGK+9f8Akq8TZFZ4VuwGEiTOLWFhAt5U4jed3Yyo457xPG8RZPi7vHTzcKjcY2FWzDLw1Is2QZFAs7MlctrHGp4FubWokqCUbm/uFSirc393U9VXrvhP/q5evdPnULiW0OHnlo76Y/brndrdzzjHdMaNqgrQopWAdeOhBFNXq8CWfR4o4Lbssslvx/KrKsSJtuyWXMmT2pKxxMkaOoSUgaJ04DrI9JqExnYRbsbyWx31eR5Hd7jZmHYkVVzmNuIEdaAkslKW0jQaJVvABZKE7yiBpVjsG0KBlUu7RbNFmXWTaZJhz2oaEOqivgcW3AlR3VDiCD1EEdYIEz3wn/1cvXunzpq9XgM+jhijPoHJ4xyHit4xd6dd7hjE/XmrRKlJLNvPOl0GOUoC0FKzqCVK00FWTBtnysKclOO5RkOSOPoQ2FXyYl4NJTrpuJQhCQTvHVRBUdBqTpU73wn/ANXL17p865mWcguCgiHjkpsk6c9cXW2Gk+s6FS/3INNXq9Kt/KMZ9GON0fm6S3Y0bditCROfUGYrBVpzrp8kfV1knzJCj1A1o2NWRGOY/b7WhwvCKwlouq63FAeMs+snUn66i8XwwWZ4z7g+m4XVSd0OhvdbYSetDSeJAPnJJKvUAEiz1ltRjmRd+P12HIyquq0rR3IUpStRSFKUoBSlKAUpSgKRtvzfwb7H8yyYL5t622qQ+wddNXtwhoftWUj9tU7k9WeNsT5KmLd8Elhq12A3WcDwKFLQqS8D6wVqH7KrnLccVkGC4js+ZUedzfJ4FpeQk6ERkuc88v6k82jX662zN8Kt+e4Pe8Vnqfj2y7QXbe8qGsNuNtuIKCUHQgEA8NQR6QRqKA+MGxHlN5PsY2wSM3jvKnIuclbl5txWUtT0LWVKB9CgVFSVaag+kFQP2d2fZ5Ztp+F2jKcflCZZ7mwH2HeojiQpKh5lJUFJUPMUkeavKVs/k28Iw3aNg9ytrQyHG4MmU/fI+USi45I1bQIiG0NNobUlLqVKUhY0UFEKKhokevcexy04lZ49psVrhWW1R97mYNvjoYYa3lFSt1CAEjVSlE6DiST56AkaUpQClKUApSlAKUpQClKUApSlAecsyHTjlxYFZ18YuG4zNyApV5KnpLgipGnnIACgfN9deja85bYv+RuVnsby4fk4t9jzcTnOdXlJ56Kn16u737q9G0BmnKDsGJXXZ4u6ZpLmwLJjkti/GVbyedacjq3kEAJUTxOmgGvHrHXV+st2jX+zwLpDUVxJrDcllSklJKFpCkkg9XAiqbtoul6g43bItlxJjMxdLrFt06DLQFsNRHFEOvOA66pQANeB6+I0q9sMNxWG2WW0NMtpCENoSEpSkDQAAdQAoDkpSlAKUpQClKUApSlAKVxvyGorZcedQ0gdanFBI/eaj+lNlH/V4HvKPjUlGUtyBKUqL6VWXtiB7yj406VWXtiB7yj41LRz5WZszwJy6OWFalXeJg8DFLzFybE8jZuguF0Lcdtt6MsFpxlKStTjbqFrIUrcICkHQ6kDR+RLyqtonKbzu/HIY1lt2P2O26LYtEVxHOyXnUc0panHHD4qGXwAkpB3zqFaJ0l+W1yc8d5QeJd+rFcbWzntpZPcjhlNpE5kakxlnXTXUkoJ6lEgkBRIr38mhi8XANjl8uV5cYtV2vF3WCzLWGXeZYSG0hSVaEaOF+mjnysWZ6Uv0C43ra7jDtuzViDCssaS9dcWZKS9OS8kIZccG9qlCFJJHi8Trx81X6sY2Y5Ls9yraTnmY2xp+3X9mX0bnTblISlqUI2hCo6Ssjm/GHjAJ3iOo9Z1PpVZe2IHvKPjTRz5WLMlKVF9KrL2xA95R8adKrL2xA95R8aaOfKxZkpSunEvMCevdizo0lX6LLyVH/wa7lQaawZgUpSsAVUMuy5+JLFptIQbgUhb8lwbzcRB6uH9JxX9FPUACpXDdSu1yH0RY7rzh0bbSVqPqA1NZDjS3JdqbuL+hl3I92vqGvFSwCBx8yU7qR6kitsbRi6j6N3rLuS0VVn9rcj+LxqDLe5+4tm8SyNDJuOjyzx14AjdSPUkAequbo/ax/02H7BHwrp5hmtkwGzm6X+4tW2FziWkrcBUpxxXkoQhIKlqPmSkEnQ8KzjL+UVZ7TGxC62yW0uw3K9qtVxfnQpDTrAEZ13RLaglYcKktgApOu9oASRWt1qkt8mdxuEMNxqfR+19mw/YJ+FOj9r7Nh+wT8Krtv2w4dc8Qm5QzfGU2SC6piVIfbWyph0EAtrbWkLSvVSQEFO8d4aA6iv5YNseHZLbbtOh3xpuPaWw7PE5pyI5FbIJC3EPJQpKSAdFEaHQ6HhUdJPmZnOjxLH0ftfZsP2CfhTo/a+zYfsE/Cssy7lF2dWAXe9YdKRcp0B6Ckt3CBJZbLciU0zvjfS2VgpWopUkkagdY4VpNozC0X693i0W+X3VOtC0NzkoaXuMrWneCOc03CrTQlIJKdRqBqKaSfMwpRbsjtdH7X2bD9gn4U6P2vs2H7BPwrnuNxi2iBJnTpDUSHGbU89IfWEIbQkaqUongAANdap2ObbsKyqNcJNvvX/CwIxmSJEuK9FaSwOt0LdQlKkf2kkimknzMy3FOzLV0ftfZsP2CfhTo/a+zYfsE/Cq5h22LD89flsWW8B5+KwJTrUmO7FXzB6ngHUJKm/7adU+uqdG5RlkyjabhmN4rOj3WHdlzRMfXEfR4jLCloUw4oJQtJUnQqTvjT0ddNJPmZFzhg77zT5GK2aUnddtMJY8xMdOo468Dpw48eFSNpvE/D1BTb0q52cfzkN1RefZH6TKid5QHnbJPDydNN1VIt+3LB7pkjdii39tye6+qK0rmHUx3nhrq22+UBpa9QRupUTqCKvVTjWmsJO64MjOFOsrPE0mLKZnRWZMd1LzDyA424g6pWkjUEH0EVy1RNl8ssKvdm1/JQpCX46R/QaeBVu/scS7p6AQPNV7qVSOZKy3f3ijzlSDpycX0HWuUQXC3SopOgfaU3r6NQR/+1kuKuKXjdtC0qQ62wllxChoUrQN1YP1KSRWx1nWVWF3HLjJusRhT1qlrLsxtoarjOkAF0J87atPG04pV42hClFEorPg6a371/X1wsXMjqqnNqXSY7tutlyi5Ns+y6LZ5eQ27HLhIdnW23t87I3Ho62kPtt/0y2og6DVWiiQOFR2VXWZtGvuzG6Qcbv0KJAyhanhc7cthaGhBfHPKQeKG95YSFLCfG83Ea7NGkszGEPx3UPsuDeQ42oKSoekEcDXJVV4YM7LhdvHeeWNpezvIrxkOfzoVnu0iDEy+z3rua3rciv3CM1Abbf7lcBTvLSo7wKVA7zemutc+SbNYed4FlNwxbHMxXfUohNlvMpMvnLjHZlIkrjNiU4pQHiKHEAEq0BIJr1BSlzXoU79p5+2w5VcNruxvIrVY8Vyy1XDnLctCrhZltLCu7mSoIQdSsoCSskAp0GupGtWjYnjtx2ZXTIMGlMTZ1taeVdrbf3mSruxD6yXW33QN0vod3us6qQpB00B01morJcSsmZ29MC/WmHeYSXA6I85hLzYWAQFbqgRqATx9ZrBPM+1n3xKjyg8RuedbHckstmZTJuT7bTjUZSwgSObeQ4pnU8BvpQUceHjcape0a9z9t2yTJsesuIZFa7gIjMlMe924wm3lNPtuKipUs6KUpKFJ1TqjQ+VWl49slwnErm3crJidmtNwbSUolQoLbTiQRoQFJAPEcKtlA4OV79OB5j2j2bINv13kv47j16xpqDil0typF8hqgrlSJQaDcZIXxUlPNKJWNUDe4E12W7hcs/y7ZXEg4fkmLJtES4xZb061OMR4C1QFNICXPJI3holQ4Hhx1Olek6UI6LG9zyxsdwC3sQMRxTKMVz9F9sr7RdU7PmrsjT0c77chCi9zBQVISUpSCQVaboA1r1PSuoJD9znKtlpCJNy08cqBLUYfpukdXqTqCrzaDVQnGEqjsjKUaMbt4E1s1jl6+5NcADzZVHgpJHWW0KWoj0jV/T6wR5qv9R2P2OPjdoj2+NvKbaBKnFnVTi1EqWtXrUolR9ZqRrfUkpSw3YLuVjztWekm5cRSlK1Goq9z2b2G5yXJIjOwZLh1W7b5DkcrOupKgggKOvnIJroeCiB2vevfflV3pW9V6i/UbFVnHBSZSPBRA7XvXvvyp4KIHa9699+VXelZ09Tj5EtNU5mUjwUQO171778qqubbHsik3HGFYrkb8WE1ckLvabhLUVuwtDvIZ0bP5TXTTXQdfGthrKdttrwm45Fsycy68TbXOjZIy9YmoiCpMucEK3GnNG16II3uJKOryhTT1OPkNNU5mT/AIKIHa9699+VPBRA7XvXvvyq70pp6nHyGmqczKR4KIHa9699+VPBRA7XvXvvyq70pp6nHyGmqczKYjZRZzwky7rMR1Ft24OpSfrCCnWrParPBscJMS3RGYUZJJDTCAhOp6zoOsnznrNdylQlVnNWk8CEpyl953FKUrUQFKUoBSlKAUpSgFZ1tXu3ey94C30B6a91X1pnuzmOc7x6pV/xuvNL3N3q3tUeV5QqxSMjktPuICGiEqKRqD5j9dRV2uVzuL8ByPdH7WmM+HnWojbSky0gfzTnOIWQg+lsoVw4KFAXilVjpPK/7bP8J+NcuAZtbdomKxL/AGiSmZb5K3UNvJbW2FFtxTavFWAoaKQocR5uHCgLFSlKAUpSgFKUoBSlKAUpSgFKUoDw1clbTdrWYbR5lhmvQn7Lfpdntpbyh2AzC5nQNrchpiuIfC9Q4S4o7wVoN0CpC5QMiynJ9rLd0yu+WyXj9mt8mNHstydYjMTFQlrcWkDQqTvtjxFeKdSSkk61t2Wcm7EswyyVkFxxtL1ykKSX3mZjrCZIQfE55ttxKHdBppvg8BpXZu2C2ey3e5SJFmmuTMuLVvnOxGZElLoS2pCOcLe8lhASpQ5w7iePFWulAYbid6vW3LKsftV1yS7WCDHwy2X5xixyzCenypQVvuKWjxi23uabg4by+OvAVsHIsaLHJuxZouLeKHp6S44RvK0nP8Tp5zXJeOTXil9iWCPLx3VNhiJgW51ic6y8zHSkJDRcQ4FrRokcFkg9Z4k1ouzbBbXs2w2Bjllgi2WyGXOZihxTgRvuKWrxlEnipSj18NdOqgLPSlKAUpSgFKUoBSlKAUpSgFKUoBVL2g2vNrjdcRcxG8QrXBjXZt6+tS0BSpcEJO+03q2vRZO7xBR1eUKulfJLbvy0NqsraHabXldgxmBecEyDu5pqJFkpQ5Ia3kAL3nyVNkHUbu6SNCDQH1tpWCcjjbBn23bZrIzDNbdZrXFlSSzamrTHdaLraNUuOqLjrmoK9UjTTTm1a66it7oBSlKAUpSgFKUoBSlKA/LjiWW1LWQlCQVEnzAVU0bWsScQlaLy0pKhqFBtwgj0+TVju35qm/Yr/wBprK8Q+idk/uLH3aaxUqQo08+UW8bb7e5nNy3LNTjGWbe/aXLwr4p2w37Jz8NPCvinbDfsnPw1CUqnr9Lq37S+E5G231fj8ib8K+KdsN+yc/DXhjlv7B7bth2s4nk2IzGiq7ON22/PJaUBGSnTclqBAKgG9UnT/toAGpr2ZSmv0urftL4Rtt9X4/I5MRy3AsGxe1Y9Z7g3FtdsjNxIzQacJShCQBqd3iTpqT5ySal/CvinbDfsnPw1CUpr9Lq37S+EbbfV+PyJvwr4p2w37Jz8NPCvinbDfsnPw1CUpr9Lq37S+EbbfV+PyLjYMstGUd0d65qJZjlIdCQQUb2umoIHXof3VL1n2zv6XZR9jD/0drQavytg47mk+9Jno6NTTU41LWurilKVE3ClKUB1Lt+apv2K/wDaayvEPonZP7ix92mtUu35qm/Yr/2msqxEhOI2Uk6AQWNSfs01Vy38uv8AL3M856a/Dh6yYpVH8Omzb/1BxX/5qN+Ov6rbls4QopVtAxZKgdCDeo2oP8dcPNlwPL6KpyvuIrJdvVmxy43ZlNlv93t9mUUXW72uCHokBQSFLStW8FKKEkKUG0r3R16HhXDfeULZbPcr5Ei2S/X5NljMzp0m1RW3GWozrXOpd3lOJ3hu6+KNVHdOiSBrWZMbG+9mWZPJd2V2DaZbchurl5t+QuyYqS03IIWpp3nAVFKSVFKmwsFKhwFX6Hs5utvyPa4uNa0R7ZebVCh2hLbjaUOFqI60UBIPiBJUlPjAD0cK3uMF9eouOnQj24ce1fPgTuQbcLJaZdmh2233bK7hdYKboxDsUZLziYZ03X176kJSgk6DU6k6gA6V+eTzmF0z7ZDY79eZC5Vxlrlc444ylpWiZLqEAoSAAQlKR1ebjxrPcLwrOdlVzxq8QsWGRiViNrsl0gtXBhl+BKioPEKWrcW2ecUDuknVOo1FT+x3IbTsc2YWHG88vtkxPIm+6ZDluuF2jpWlDkt5aFA7/jJIPWPQRwIIGJRjm2jj9MxUpwVNqni7rtfTfDuNppVI8OezfQHwg4toeGvfqN+Op7G8zx/MmHnsfvttvrLKgh1y2y25CW1EagKKCdDp5jWlxa3opOnOKu0ywbO/pdlH2MP/AEdrQaz7Z39Lso+xh/6O1oNen/TD/GP/AFR9ByP8tT9SFKUrBcFKUoDqXb81TfsV/wC01leIfRKyf3Fj7tNa642l5tSFgKQoFJB84NVNGybE20JQizNpSkaBIdcAA9HlVipThWp5kpNY33X96ObluR65GMc61uwrve+L+rM+zFO98X9WZ9mKsfgpxTshHtXPxU8FOKdkI9q5+KqeoUusfsr4jkbFfWeHzIMAJAAGgHAAV/am/BTinZCPaufip4KcU7IR7Vz8VNQpdY/ZXxDYj6zw+ZCVxOxWX1bzjLbitNNVJBNWDwU4p2Qj2rn4qeCnFOyEe1c/FTUKXWP2V8Q2I+s8PmVzvfF/VmfZiuRphtgENtpbB6whIFT/AIKcU7IR7Vz8VPBTinZCPaufipqFLrH7K+IbEfWeHzIvZ39Lso+xh/6O1oNRFgxO04v3R3rhIiGQUl0pJJXu66akk9Wp/fUvV+VsFHckl3JI9HRp6GnGne9lYUpSom4UpSgFKUoBSlKAUpSgFKUoBSlKAUpSgFKUoD//2Q==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "builder.add_node(\"generate\", generate)  # generation solution\n",
    "builder.add_node(\"check_code\", code_check)  # check code\n",
    "\n",
    "# Build graph\n",
    "builder.set_entry_point(\"generate\")\n",
    "builder.add_edge(\"generate\", \"check_code\")\n",
    "builder.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Codestral Self Correction Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTAQIierN50R",
    "outputId": "462d9f19-eed3-4d7b-ca2b-52576f1ad5dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Write a Python program that prints 'Hello, World!' to the console.\n",
      "---GENERATING CODE SOLUTION---\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here is my attempt to solve the problem: To print 'Hello, World!' to the console in Python, we can simply use the print function. \n",
      " Imports:  \n",
      " Code: print('Hello, World!')\n",
      "---CHECKING CODE---\n",
      "Hello, World!\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "code(prefix=\"To print 'Hello, World!' to the console in Python, we can simply use the print function.\", imports='', code=\"print('Hello, World!')\", description='Schema for code solutions to questions about LCEL.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "_printed = set()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Checkpoints are accessed by thread_id\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "\n",
    "question = \"Write a Python program that prints 'Hello, World!' to the console.\"\n",
    "events = graph.stream(\n",
    "    {\"messages\": [(\"user\", question)], \"iterations\": 0}, config, stream_mode=\"values\"\n",
    ")\n",
    "for event in events:\n",
    "    _print_event(event, _printed)\n",
    "\n",
    "event['generation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuuVMU99OUYX"
   },
   "source": [
    "### LangSmith trace of a single run\n",
    "\n",
    "https://smith.langchain.com/public/92073ab4-c7ba-41f2-b918-76232bf1edc9/r"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
