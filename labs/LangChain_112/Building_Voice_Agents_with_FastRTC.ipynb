{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7f30a0d9-7ef1-4135-a3ea-085c1f83bc0b",
      "metadata": {
        "id": "7f30a0d9-7ef1-4135-a3ea-085c1f83bc0b"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aimug-org/austin_langchain/blob/main/labs/LangChain_112/Building_Voice_Agents_with_FastRTC.ipynb)  \n",
        "\n",
        "# Building Voice Agents with FastRTC\n",
        "\n",
        "This notebook will walk you through setting up and testing a simple FastRTC server.  \n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. `uv` from [astral.sh](https://docs.astral.sh/uv/) to manage python environments. (Already present in Google Colab)\n",
        "2. OpenAI API Key from https://platform.openai.com/settings/organization/api-keys available as environment variable `OPENAI_API_KEY` or as Google Colab Secrets\n",
        "3. (Optional for FastRTC Client WebPage test) Ngrok AuthToken from https://dashboard.ngrok.com/get-started/your-authtoken available as environment variable `NGROK_AUTH_TOKEN` or as Google Colab Secrets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69438025-932f-47a5-9782-b79744604bdf",
      "metadata": {
        "id": "69438025-932f-47a5-9782-b79744604bdf"
      },
      "source": [
        "## Initialize UV project and virtual environment\n",
        "\n",
        "Initialize a new `uv` project with `uv init`.  \n",
        "Pin Python to version 3.13  \n",
        "Create a new virtual environment for the project.\n",
        "\n",
        "Note: `export VIRTUAL_ENV=` is needed only within Jupyter notebooks and can be excluded otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7025c387-8e83-431d-8458-18d284b55e01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7025c387-8e83-431d-8458-18d284b55e01",
        "outputId": "ad18ef86-fe31-4cb8-9b7a-24b41721aded"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated `.python-version` from `3.11` -> `3.13`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Initialized project `content`\n",
            "warning: No interpreter found for Python 3.13 in managed installations or search path\n",
            "warning: The `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\n",
            "Downloading cpython-3.13.2-linux-x86_64-gnu (20.4MiB)\n",
            " Downloaded cpython-3.13.2-linux-x86_64-gnu\n",
            "Using CPython 3.13.2\n",
            "Creating virtual environment at: .venv\n",
            "Activate with: source .venv/bin/activate\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=\n",
        "uv init\n",
        "uv python pin 3.13\n",
        "uv venv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01b3cf5-5282-4c4d-abbc-a39eb6d75639",
      "metadata": {
        "id": "a01b3cf5-5282-4c4d-abbc-a39eb6d75639"
      },
      "source": [
        "## Add FastRTC to the project dependencies\n",
        "\n",
        "Note: `export VIRTUAL_ENV=.venv` is needed only within Jupyter notebooks and can be excluded otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d58e4c0f-2f02-4bb3-ac1d-41224cfe9579",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d58e4c0f-2f02-4bb3-ac1d-41224cfe9579",
        "outputId": "5bdb8690-96d3-4882-a9c8-069420f6e9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolved 135 packages in 120ms\n",
            "Downloading babel (9.7MiB)\n",
            "Downloading scipy (35.5MiB)\n",
            "Downloading scikit-learn (12.6MiB)\n",
            "Downloading numpy (15.4MiB)\n",
            "Downloading av (33.5MiB)\n",
            "Downloading onnxruntime (15.3MiB)\n",
            "Downloading pandas (12.1MiB)\n",
            "Downloading ruff (11.0MiB)\n",
            "Downloading llvmlite (40.4MiB)\n",
            "Downloading gradio (51.6MiB)\n",
            "Downloading espeakng-loader (9.6MiB)\n",
            " Downloaded babel\n",
            " Downloaded ruff\n",
            " Downloaded espeakng-loader\n",
            " Downloaded scikit-learn\n",
            " Downloaded numpy\n",
            " Downloaded pandas\n",
            " Downloaded onnxruntime\n",
            " Downloaded av\n",
            " Downloaded llvmlite\n",
            " Downloaded scipy\n",
            " Downloaded gradio\n",
            "Prepared 11 packages in 9.91s\n",
            "Installed 95 packages in 333ms\n",
            " + aiofiles==24.1.0\n",
            " + aioice==0.10.1\n",
            " + aiortc==1.11.0\n",
            " + attrs==25.3.0\n",
            " + audioop-lts==0.2.1\n",
            " + audioread==3.0.1\n",
            " + av==14.3.0\n",
            " + babel==2.17.0\n",
            " + cffi==1.17.1\n",
            " + click==8.1.8\n",
            " + colorama==0.4.6\n",
            " + coloredlogs==15.0.1\n",
            " + colorlog==6.9.0\n",
            " + cryptography==44.0.3\n",
            " + csvw==3.5.1\n",
            " + decorator==5.2.1\n",
            " + dlinfo==2.0.0\n",
            " + dnspython==2.7.0\n",
            " + espeakng-loader==0.2.4\n",
            " + fastapi==0.115.12\n",
            " + fastrtc==0.0.23\n",
            " + fastrtc-moonshine-onnx==20241016\n",
            " + ffmpy==0.5.0\n",
            " + filelock==3.18.0\n",
            " + flatbuffers==25.2.10\n",
            " + fsspec==2025.3.2\n",
            " + google-crc32c==1.7.1\n",
            " + gradio==5.29.0\n",
            " + gradio-client==1.10.0\n",
            " + groovy==0.1.2\n",
            " + huggingface-hub==0.30.2\n",
            " + humanfriendly==10.0\n",
            " + ifaddr==0.2.0\n",
            " + isodate==0.7.2\n",
            " + jinja2==3.1.6\n",
            " + joblib==1.5.0\n",
            " + jsonschema==4.23.0\n",
            " + jsonschema-specifications==2025.4.1\n",
            " + kokoro-onnx==0.4.8\n",
            " + language-tags==1.2.0\n",
            " + lazy-loader==0.4\n",
            " + librosa==0.11.0\n",
            " + llvmlite==0.44.0\n",
            " + markdown-it-py==3.0.0\n",
            " + markupsafe==3.0.2\n",
            " + mdurl==0.1.2\n",
            " + mpmath==1.3.0\n",
            " + msgpack==1.1.0\n",
            " + numba==0.61.2\n",
            " + numpy==2.2.5\n",
            " + onnxruntime==1.21.1\n",
            " + pandas==2.2.3\n",
            " + phonemizer-fork==3.3.1\n",
            " + pillow==11.2.1\n",
            " + platformdirs==4.3.7\n",
            " + pooch==1.8.2\n",
            " + protobuf==6.30.2\n",
            " + pycparser==2.22\n",
            " + pydub==0.25.1\n",
            " + pyee==13.0.0\n",
            " + pygments==2.19.1\n",
            " + pylibsrtp==0.12.0\n",
            " + pyopenssl==25.0.0\n",
            " + pyparsing==3.2.3\n",
            " + python-dateutil==2.9.0.post0\n",
            " + python-multipart==0.0.20\n",
            " + pytz==2025.2\n",
            " + rdflib==7.1.4\n",
            " + referencing==0.36.2\n",
            " + rfc3986==1.5.0\n",
            " + rich==14.0.0\n",
            " + rpds-py==0.24.0\n",
            " + ruff==0.11.8\n",
            " + safehttpx==0.1.6\n",
            " + scikit-learn==1.6.1\n",
            " + scipy==1.15.2\n",
            " + segments==2.3.0\n",
            " + semantic-version==2.10.0\n",
            " + shellingham==1.5.4\n",
            " + six==1.17.0\n",
            " + soundfile==0.13.1\n",
            " + soxr==0.5.0.post1\n",
            " + standard-aifc==3.13.0\n",
            " + standard-chunk==3.13.0\n",
            " + standard-sunau==3.13.0\n",
            " + starlette==0.46.2\n",
            " + sympy==1.14.0\n",
            " + threadpoolctl==3.6.0\n",
            " + tokenizers==0.21.1\n",
            " + tomlkit==0.13.2\n",
            " + typer==0.15.3\n",
            " + tzdata==2025.2\n",
            " + uritemplate==4.1.1\n",
            " + uvicorn==0.34.2\n",
            " + websockets==15.0.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=.venv\n",
        "uv add fastrtc[vad,tts,stt]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "395233ae-ca85-4ac5-acbb-cca70c9ce57f",
      "metadata": {
        "id": "395233ae-ca85-4ac5-acbb-cca70c9ce57f"
      },
      "source": [
        "## Echo Server from fastrtc.org [QuickStart](https://fastrtc.org/#__tabbed_1_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d057c1a0-715c-47ba-9f5e-f86a5f75f5f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d057c1a0-715c-47ba-9f5e-f86a5f75f5f8",
        "outputId": "a423599a-77cf-4d75-bb69-3752eacd9756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause\n",
        "import numpy as np\n",
        "\n",
        "def echo(audio: tuple[int, np.ndarray]):\n",
        "    # The function will be passed the audio until the user pauses\n",
        "    # Implement any iterator that yields audio\n",
        "    # See \"LLM Voice Chat\" for a more complete example\n",
        "    yield audio\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(echo),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    # below rtc_configuration needed to work around potential firewall issues\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0860819d-f42d-4568-9aa1-17c5df22487e",
      "metadata": {
        "id": "0860819d-f42d-4568-9aa1-17c5df22487e"
      },
      "source": [
        "## Make it a Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "69d97832-2974-43b2-8004-c1f2c47430e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69d97832-2974-43b2-8004-c1f2c47430e7",
        "outputId": "6533fe5c-d9d9-4dc1-d555-6ba24fa1c330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile -a app.py\n",
        "stream.ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badd1525-f0ec-4757-a8ff-e7fd7fba42a7",
      "metadata": {
        "id": "badd1525-f0ec-4757-a8ff-e7fd7fba42a7"
      },
      "source": [
        "## Running Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8ea939b3-ac06-425f-95ef-14fdede68c62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ea939b3-ac06-425f-95ef-14fdede68c62",
        "outputId": "afd3be60-97a7-4008-92db-d19dfe55333f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rsilero_vad.onnx:   0% 0.00/1.81M [00:00<?, ?B/s]\rsilero_vad.onnx: 100% 1.81M/1.81M [00:00<00:00, 36.3MB/s]\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://82bc0ec9065038c8bd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Invalid candidate format: candidate:0 1 UDP 2122187007 192.168.1.22 49363 typ host\n",
            "Invalid candidate format: candidate:2 1 UDP 2122252543 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 59115 typ host\n",
            "Invalid candidate format: candidate:0 1 UDP 2122187007 192.168.1.22 52226 typ host\n",
            "Invalid candidate format: candidate:2 1 UDP 2122252543 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 62591 typ host\n",
            "Invalid candidate format: candidate:2 2 UDP 2122252542 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 51254 typ host\n",
            "Invalid candidate format: candidate:0 2 UDP 2122187006 192.168.1.22 64623 typ host\n",
            "Invalid candidate format: \n",
            "Invalid candidate format: \n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://82bc0ec9065038c8bd.gradio.live\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv GRADIO_SHARE=\"True\" uv run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "977b6a57-a9ce-4c76-bb84-d1000d84ee58",
      "metadata": {
        "id": "977b6a57-a9ce-4c76-bb84-d1000d84ee58"
      },
      "source": [
        "## FastRTC using FastAPI backend and HTML frontend\n",
        "\n",
        "`nest-asyncio` and `pyngrok` are only needed for testing from within Google Colab.  \n",
        "These can be excluded otherwise."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=.venv\n",
        "uv add nest-asyncio pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCpRlPw8OGhN",
        "outputId": "74a0600d-7ac7-4420-ad6f-a84e1001e882"
      },
      "id": "KCpRlPw8OGhN",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolved 135 packages in 1ms\n",
            "Audited 133 packages in 0.08ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dee9031c-5bb0-4c7d-875e-6a62c7d98888",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee9031c-5bb0-4c7d-875e-6a62c7d98888",
        "outputId": "1f16dc65-934d-4541-e439-eca807d6a674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause\n",
        "import numpy as np\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import HTMLResponse\n",
        "import os\n",
        "\n",
        "# Below imports are only needed to get this example to work within Google Colab\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Below two lines are only needed to get this to work within Google Colab\n",
        "NGROK_AUTH_TOKEN = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "def echo(audio: tuple[int, np.ndarray]):\n",
        "    # The function will be passed the audio until the user pauses\n",
        "    # Implement any iterator that yields audio\n",
        "    # See \"LLM Voice Chat\" for a more complete example\n",
        "    yield audio\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(echo),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")\n",
        "app = FastAPI()\n",
        "stream.mount(app)\n",
        "\n",
        "# Optional: Add routes\n",
        "@app.get(\"/\")\n",
        "async def _():\n",
        "    return HTMLResponse(content=open(\"index.html\").read())\n",
        "\n",
        "# Below lines are only needed to get this example to work within Google Colab\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83b81b6-e2ef-4a97-ac53-27befdc85b37",
      "metadata": {
        "id": "b83b81b6-e2ef-4a97-ac53-27befdc85b37"
      },
      "source": [
        "### Client Html Page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9d0dcd44-18d0-41a6-9386-488f7c9abef1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d0dcd44-18d0-41a6-9386-488f7c9abef1",
        "outputId": "ef52770d-2990-4d46-807c-499b8c81dcb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting index.html\n"
          ]
        }
      ],
      "source": [
        "%%writefile index.html\n",
        "<html>\n",
        "\n",
        "<head>\n",
        "  <title>FastRTC Client Demo</title>\n",
        "  <script src=\"https://cdn.jsdelivr.net/gh/lalanikarim/fastrtc-client@v0.1.2/fastrtc-client.js\"></script>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "  <h1>FastRTC Echo Server</h1>\n",
        "  <button id=\"start\">Connect</button>\n",
        "  <button id=\"stop\" style=\"display: none\">Disconnect</button>\n",
        "  <h3>Logs</h3>\n",
        "  <pre class=\"logs\"></pre>\n",
        "  <audio></audio>\n",
        "  <script defer>\n",
        "    let logs = document.querySelector(\"pre\")\n",
        "    let startButton = document.querySelector(\"button#start\")\n",
        "    let stopButton = document.querySelector(\"button#stop\")\n",
        "    let client = FastRTCClient({\n",
        "      additional_outputs_url: null,\n",
        "      // below rtc_config is needed to work around potential firewall issues\n",
        "      rtc_config: {\n",
        "        iceServers: [\n",
        "          {\n",
        "            urls: [\"stun:stun.l.google.com:19302\"]\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    })\n",
        "    client.onConnecting(() => {\n",
        "      logs.innerText += \"Connecting to server.\\n\"\n",
        "      startButton.style.display = \"none\"\n",
        "      stopButton.style.display = \"block\"\n",
        "    })\n",
        "    client.onConnected(() => {\n",
        "      logs.innerText += \"Connected to server.\\n\"\n",
        "    })\n",
        "    client.onReadyToConnect(() => {\n",
        "      logs.innerText += \"Not connected to server.\\n\"\n",
        "      startButton.style.display = \"block\"\n",
        "      stopButton.style.display = \"none\"\n",
        "    })\n",
        "    client.onErrorReceived((error) => {\n",
        "      logs.innerText += `serverError received: ${error}\\n`\n",
        "    })\n",
        "    client.onPauseDetectedReceived(() => {\n",
        "      logs.innerText += `pause detected event received. response will start now.\\n`\n",
        "    })\n",
        "    client.onResponseStarting(() => {\n",
        "      logs.innerText += `response starting event received. audio will start playing now.\\n`\n",
        "    })\n",
        "    client.setShowErrorCallback((error) => {\n",
        "      logs.innerText += `showError received: ${error}\\n`\n",
        "    })\n",
        "    startButton.addEventListener(\"click\", () => client.start())\n",
        "    stopButton.addEventListener(\"click\", () => client.stop())\n",
        "  </script>\n",
        "</body>\n",
        "\n",
        "</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33e2ea3-19d5-402c-95e5-5a1c9efcf222",
      "metadata": {
        "id": "d33e2ea3-19d5-402c-95e5-5a1c9efcf222"
      },
      "source": [
        "### Testing webpage locally\n",
        "\n",
        "In order to test the web client locally, you can run the below command.  \n",
        "WebRTC will work locally over http over localhost.  \n",
        "In order to use another domain, WebRTC will only work over https."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "7S0v6hKxPdIv"
      },
      "id": "7S0v6hKxPdIv",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8307f37b-590d-47a0-9151-946f68024d09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8307f37b-590d-47a0-9151-946f68024d09",
        "outputId": "0411542d-0669-476c-b097-6d2ba0046dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "Public URL: https://b24f-34-127-66-123.ngrok-free.app\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1623\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:\t  Visit \u001b[36mhttps://fastrtc.org/userguide/api/\u001b[0m for WebRTC or Websocket API docs.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     76.85.61.16:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     76.85.61.16:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     76.85.61.16:0 - \"\u001b[1mPOST /webrtc/offer HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m1623\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ~~~~~~~~~~^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py\", line 712, in run_until_complete\n",
            "    self.run_forever()\n",
            "    ~~~~~~~~~~~~~~~~^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py\", line 683, in run_forever\n",
            "    self._run_once()\n",
            "    ~~~~~~~~~~~~~~^^\n",
            "  File \"/content/.venv/lib/python3.13/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "    ~~~~~~~~~~~^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/events.py\", line 89, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/.venv/lib/python3.13/site-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "         ~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/contextlib.py\", line 148, in __exit__\n",
            "    next(self.gen)\n",
            "    ~~~~^^^^^^^^^^\n",
            "  File \"/content/.venv/lib/python3.13/site-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/runners.py\", line 157, in _on_sigint\n",
            "    raise KeyboardInterrupt()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/.venv/lib/python3.13/site-packages/starlette/routing.py\", line 699, in lifespan\n",
            "    await receive()\n",
            "  File \"/content/.venv/lib/python3.13/site-packages/uvicorn/lifespan/on.py\", line 137, in receive\n",
            "    return await self.receive_queue.get()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/queues.py\", line 186, in get\n",
            "    await getter\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py\", line 286, in __await__\n",
            "    yield self  # This tells Task to wait for completion.\n",
            "    ^^^^^^^^^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/tasks.py\", line 375, in __wakeup\n",
            "    future.result()\n",
            "    ~~~~~~~~~~~~~^^\n",
            "  File \"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/asyncio/futures.py\", line 194, in result\n",
            "    raise self._make_cancelled_error()\n",
            "asyncio.exceptions.CancelledError\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv NGROK_AUTH_TOKEN={ngrok_auth_token} uv run --with uvicorn uvicorn app:app"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b308baca-fe39-486c-84b7-1efe914be73a",
      "metadata": {
        "id": "b308baca-fe39-486c-84b7-1efe914be73a"
      },
      "source": [
        "## Echo Server with STT and TTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8cd7f487-df11-43b4-9c37-7ad236851600",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cd7f487-df11-43b4-9c37-7ad236851600",
        "outputId": "18f0112a-6716-4464-85b6-cc62ab4532ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause, get_stt_model, get_tts_model\n",
        "import numpy as np\n",
        "\n",
        "stt_model = get_stt_model() # Moonshine\n",
        "tts_model = get_tts_model() # Kokoro\n",
        "\n",
        "def echo(audio: tuple[int, np.ndarray]):\n",
        "    text = stt_model.stt(audio)\n",
        "    for audio_chunk in tts_model.stream_tts_sync(text):\n",
        "        yield audio_chunk\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(echo),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")\n",
        "stream.ui.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f82ef0f6-a1b3-484a-9e0b-5098bf95a529",
      "metadata": {
        "id": "f82ef0f6-a1b3-484a-9e0b-5098bf95a529"
      },
      "source": [
        "## Running Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a8a103f1-ce17-4155-b7af-125ef584daf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8a103f1-ce17-4155-b7af-125ef584daf3",
        "outputId": "08466581-ffc1-461c-c4de-c3cd7bfb5b6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_model.onnx: 100% 80.8M/80.8M [00:00<00:00, 254MB/s]\n",
            "decoder_model_merged.onnx: 100% 166M/166M [00:00<00:00, 245MB/s]\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up STT model.\n",
            "\u001b[32mINFO\u001b[0m:\t  STT model warmed up.\n",
            "kokoro-v1.0.onnx: 100% 326M/326M [00:01<00:00, 233MB/s]\n",
            "voices-v1.0.bin: 100% 28.2M/28.2M [00:00<00:00, 116MB/s] \n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://3cc7934a9a3fa4efaf.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Invalid candidate format: candidate:0 1 UDP 2122187007 192.168.1.22 56161 typ host\n",
            "Invalid candidate format: candidate:2 1 UDP 2122252543 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 61019 typ host\n",
            "Invalid candidate format: \n",
            "Invalid candidate format: candidate:2 2 UDP 2122252542 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 59885 typ host\n",
            "Invalid candidate format: candidate:0 2 UDP 2122187006 192.168.1.22 53574 typ host\n",
            "Invalid candidate format: candidate:0 1 UDP 2122187007 192.168.1.22 57785 typ host\n",
            "Invalid candidate format: candidate:2 1 UDP 2122252543 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 53721 typ host\n",
            "Invalid candidate format: \n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3cc7934a9a3fa4efaf.gradio.live\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv GRADIO_SHARE=\"True\" uv run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b172de2-29bd-40ea-9a1a-fb143f721f5a",
      "metadata": {
        "id": "0b172de2-29bd-40ea-9a1a-fb143f721f5a"
      },
      "source": [
        "## LangChain App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b7bc6578-0543-4d9a-a620-7f7829835435",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7bc6578-0543-4d9a-a620-7f7829835435",
        "outputId": "c3263bcf-f802-4249-decb-a15dd2c8cf20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Resolved 135 packages in 0.89ms\n",
            "Audited 133 packages in 0.06ms\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export VIRTUAL_ENV=.venv\n",
        "uv add langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "62fb6cee-84d7-4fb8-9fd6-39d37ad4ea17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62fb6cee-84d7-4fb8-9fd6-39d37ad4ea17",
        "outputId": "36a62831-a26e-4afd-aa85-ffd7d8163536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from fastrtc import Stream, ReplyOnPause, get_stt_model, get_tts_model\n",
        "import numpy as np\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "stt_model = get_stt_model() # Moonshine\n",
        "tts_model = get_tts_model() # Kokoro\n",
        "\n",
        "model = init_chat_model(\"openai:gpt-4.1-nano-2025-04-14\")\n",
        "\n",
        "def talk(audio: tuple[int, np.ndarray]):\n",
        "    prompt = stt_model.stt(audio)\n",
        "    response = model.invoke(prompt)\n",
        "    for audio_chunk in tts_model.stream_tts_sync(response.content):\n",
        "        yield audio_chunk\n",
        "\n",
        "stream = Stream(\n",
        "    handler=ReplyOnPause(talk),\n",
        "    modality=\"audio\",\n",
        "    mode=\"send-receive\",\n",
        "    rtc_configuration={\n",
        "        \"iceServers\": [{ \"urls\": [\"stun:stun.l.google.com:19302\"] }]\n",
        "    }\n",
        ")\n",
        "stream.ui.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3f1dbf06-cf8f-4a3e-be8f-25404fe23950",
      "metadata": {
        "id": "3f1dbf06-cf8f-4a3e-be8f-25404fe23950"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5c296e4d-b2ae-4433-9303-8b61fdb1e1ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5c296e4d-b2ae-4433-9303-8b61fdb1e1ae",
        "outputId": "ad64659c-2419-4d61-c826-7525fff9d231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:\t  Warming up STT model.\n",
            "\u001b[32mINFO\u001b[0m:\t  STT model warmed up.\n",
            "\u001b[32mINFO\u001b[0m:\t  Warming up VAD model.\n",
            "\u001b[32mINFO\u001b[0m:\t  VAD model warmed up.\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://43e9d5d6836775191a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Invalid candidate format: candidate:0 1 UDP 2122187007 192.168.1.22 63385 typ host\n",
            "Invalid candidate format: candidate:2 1 UDP 2122252543 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 58168 typ host\n",
            "Invalid candidate format: candidate:2 1 UDP 2122252543 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 62899 typ host\n",
            "Invalid candidate format: candidate:0 2 UDP 2122187006 192.168.1.22 64405 typ host\n",
            "Invalid candidate format: candidate:0 1 UDP 2122187007 192.168.1.22 49197 typ host\n",
            "Invalid candidate format: candidate:2 2 UDP 2122252542 fd51:78ea:3da4:9383:8fa:ffa:56b:9e5e 51197 typ host\n",
            "Invalid candidate format: \n",
            "Invalid candidate format: \n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/blocks.py\"\u001b[0m, line \u001b[35m3019\u001b[0m, in \u001b[35mblock_thread\u001b[0m\n",
            "    \u001b[31mtime.sleep\u001b[0m\u001b[1;31m(0.1)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
            "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/content/app.py\"\u001b[0m, line \u001b[35m24\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    \u001b[31mstream.ui.launch\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/fastrtc/stream.py\"\u001b[0m, line \u001b[35m261\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
            "    return callable(*args, **kwargs)\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/blocks.py\"\u001b[0m, line \u001b[35m2925\u001b[0m, in \u001b[35mlaunch\u001b[0m\n",
            "    \u001b[31mself.block_thread\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/blocks.py\"\u001b[0m, line \u001b[35m3023\u001b[0m, in \u001b[35mblock_thread\u001b[0m\n",
            "    \u001b[31mself.server.close\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/content/.venv/lib/python3.13/site-packages/gradio/http_server.py\"\u001b[0m, line \u001b[35m69\u001b[0m, in \u001b[35mclose\u001b[0m\n",
            "    \u001b[31mself.thread.join\u001b[0m\u001b[1;31m(timeout=5)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/root/.local/share/uv/python/cpython-3.13.2-linux-x86_64-gnu/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m1092\u001b[0m, in \u001b[35mjoin\u001b[0m\n",
            "    \u001b[31mself._handle.join\u001b[0m\u001b[1;31m(timeout)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
            "\u001b[1;35mKeyboardInterrupt\u001b[0m\n",
            "Killing tunnel 127.0.0.1:7860 <> https://43e9d5d6836775191a.gradio.live\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!VIRTUAL_ENV=.venv OPENAI_API_KEY={api_key} GRADIO_SHARE=\"True\" uv run app.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7fbe346e-a937-42b9-a3e1-7f324bee11aa",
      "metadata": {
        "id": "7fbe346e-a937-42b9-a3e1-7f324bee11aa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}